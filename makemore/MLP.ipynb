{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n"
     ]
    }
   ],
   "source": [
    "names = open('names.txt').read().splitlines()\n",
    "print(names[:10]) # 10 first names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary\n",
    "chars = sorted(list(set(''.join(names))))\n",
    "stoi = {s: i + 1 for i, s in enumerate(chars)} # string to index\n",
    "stoi['.'] = 0 # padding\n",
    "itos = {i: s for s, i in stoi.items()} # index to string\n",
    "print(itos) # index to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... -> e\n",
      "..e -> m\n",
      ".em -> m\n",
      "emm -> a\n",
      "mma -> .\n",
      "olivia\n",
      "... -> o\n",
      "..o -> l\n",
      ".ol -> i\n",
      "oli -> v\n",
      "liv -> i\n",
      "ivi -> a\n",
      "via -> .\n",
      "ava\n",
      "... -> a\n",
      "..a -> v\n",
      ".av -> a\n",
      "ava -> .\n",
      "isabella\n",
      "... -> i\n",
      "..i -> s\n",
      ".is -> a\n",
      "isa -> b\n",
      "sab -> e\n",
      "abe -> l\n",
      "bel -> l\n",
      "ell -> a\n",
      "lla -> .\n",
      "sophia\n",
      "... -> s\n",
      "..s -> o\n",
      ".so -> p\n",
      "sop -> h\n",
      "oph -> i\n",
      "phi -> a\n",
      "hia -> .\n",
      "torch.Size([32, 3]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Block size and batch size\n",
    "block_size = 3 # how many characters to predict\n",
    "# Training and validation split\n",
    "X = []\n",
    "Y = []\n",
    "for name in names[:5]:\n",
    "    print(name)\n",
    "    context = [0] * block_size # padding\n",
    "    for ch in name + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '->', itos[ix])\n",
    "        context = context[1:] + [ix] # shift the context\n",
    "# Convert to tensors\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "print(X.shape, Y.shape) # (number of examples, block size), (number of examples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype # (number of examples, block size), (number of examples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27, 2)) # 27 characters, 2-dimensional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is C meaning?\n",
    "# C is a matrix of shape (27, 2) where each row corresponds to a character in the vocabulary.\n",
    "# The first row corresponds to the padding character (index 0), and the rest correspond to the characters in the vocabulary.\n",
    "# Each character is represented by a 2-dimensional vector, which can be thought of as an embedding for that character.\n",
    "# The embedding is initialized randomly, and during training, it will be updated to capture the relationships between characters.\n",
    "# The embedding allows the model to learn a representation of characters that captures their similarities and differences.\n",
    "# The embedding can be used as input to a neural network, allowing the model to learn patterns in the data.\n",
    "# The embedding can also be used for visualization, allowing us to see how characters are related in the embedding space.\n",
    "# The embedding can be used for various tasks such as character-level language modeling, text generation, and more.\n",
    "# Are they weights or biases?\n",
    "# They are weights. In a neural network, weights are parameters that are learned during training.\n",
    "# Weights are used to transform the input data into a different representation, allowing the model to learn patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1676, -0.3755])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[5] # embedding for character '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27) # one-hot encoding for character '5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27).shape # one-hot encoding for character '5' with 27 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1676, -0.3755])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[5] # embedding for character '5' using the embedding matrix C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.one_hot(torch.tensor(5), num_classes=27) @ C # embedding for character '5' using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1676, -0.3755])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C # one-hot encoding for character '5' with 27 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C[5] == F.one_hot(torch.tensor(5), num_classes=27).float() @ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1676, -0.3755],\n",
       "        [ 0.1135,  0.5526],\n",
       "        [-1.1090, -1.8569],\n",
       "        [-1.1090, -1.8569],\n",
       "        [-1.1090, -1.8569],\n",
       "        [-1.1090, -1.8569],\n",
       "        [-1.1090, -1.8569]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[torch.tensor([5, 6, 7,7,7,7,7])] # embedding for characters '5', '6', '7' using the embedding matrix C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5505,  0.9653],\n",
       "         [-0.5505,  0.9653],\n",
       "         [-0.5505,  0.9653]],\n",
       "\n",
       "        [[-0.5505,  0.9653],\n",
       "         [-0.5505,  0.9653],\n",
       "         [ 0.1676, -0.3755]],\n",
       "\n",
       "        [[-0.5505,  0.9653],\n",
       "         [ 0.1676, -0.3755],\n",
       "         [-0.3721, -0.7949]],\n",
       "\n",
       "        [[ 0.1676, -0.3755],\n",
       "         [-0.3721, -0.7949],\n",
       "         [-0.3721, -0.7949]],\n",
       "\n",
       "        [[-0.3721, -0.7949],\n",
       "         [-0.3721, -0.7949],\n",
       "         [ 0.5172, -1.0980]],\n",
       "\n",
       "        [[-0.5505,  0.9653],\n",
       "         [-0.5505,  0.9653],\n",
       "         [-0.5505,  0.9653]],\n",
       "\n",
       "        [[-0.5505,  0.9653],\n",
       "         [-0.5505,  0.9653],\n",
       "         [ 0.6375,  0.2702]],\n",
       "\n",
       "        [[-0.5505,  0.9653],\n",
       "         [ 0.6375,  0.2702],\n",
       "         [ 0.1712, -0.7044]],\n",
       "\n",
       "        [[ 0.6375,  0.2702],\n",
       "         [ 0.1712, -0.7044],\n",
       "         [ 0.0332, -0.0345]],\n",
       "\n",
       "        [[ 0.1712, -0.7044],\n",
       "         [ 0.0332, -0.0345],\n",
       "         [ 0.5857,  0.6051]],\n",
       "\n",
       "        [[ 0.0332, -0.0345],\n",
       "         [ 0.5857,  0.6051],\n",
       "         [ 0.0332, -0.0345]],\n",
       "\n",
       "        [[ 0.5857,  0.6051],\n",
       "         [ 0.0332, -0.0345],\n",
       "         [ 0.5172, -1.0980]],\n",
       "\n",
       "        [[-0.5505,  0.9653],\n",
       "         [-0.5505,  0.9653],\n",
       "         [-0.5505,  0.9653]],\n",
       "\n",
       "        [[-0.5505,  0.9653],\n",
       "         [-0.5505,  0.9653],\n",
       "         [ 0.5172, -1.0980]],\n",
       "\n",
       "        [[-0.5505,  0.9653],\n",
       "         [ 0.5172, -1.0980],\n",
       "         [ 0.5857,  0.6051]],\n",
       "\n",
       "        [[ 0.5172, -1.0980],\n",
       "         [ 0.5857,  0.6051],\n",
       "         [ 0.5172, -1.0980]],\n",
       "\n",
       "        [[-0.5505,  0.9653],\n",
       "         [-0.5505,  0.9653],\n",
       "         [-0.5505,  0.9653]],\n",
       "\n",
       "        [[-0.5505,  0.9653],\n",
       "         [-0.5505,  0.9653],\n",
       "         [ 0.0332, -0.0345]],\n",
       "\n",
       "        [[-0.5505,  0.9653],\n",
       "         [ 0.0332, -0.0345],\n",
       "         [ 0.4970, -0.4172]],\n",
       "\n",
       "        [[ 0.0332, -0.0345],\n",
       "         [ 0.4970, -0.4172],\n",
       "         [ 0.5172, -1.0980]],\n",
       "\n",
       "        [[ 0.4970, -0.4172],\n",
       "         [ 0.5172, -1.0980],\n",
       "         [-0.5991, -0.7920]],\n",
       "\n",
       "        [[ 0.5172, -1.0980],\n",
       "         [-0.5991, -0.7920],\n",
       "         [ 0.1676, -0.3755]],\n",
       "\n",
       "        [[-0.5991, -0.7920],\n",
       "         [ 0.1676, -0.3755],\n",
       "         [ 0.1712, -0.7044]],\n",
       "\n",
       "        [[ 0.1676, -0.3755],\n",
       "         [ 0.1712, -0.7044],\n",
       "         [ 0.1712, -0.7044]],\n",
       "\n",
       "        [[ 0.1712, -0.7044],\n",
       "         [ 0.1712, -0.7044],\n",
       "         [ 0.5172, -1.0980]],\n",
       "\n",
       "        [[-0.5505,  0.9653],\n",
       "         [-0.5505,  0.9653],\n",
       "         [-0.5505,  0.9653]],\n",
       "\n",
       "        [[-0.5505,  0.9653],\n",
       "         [-0.5505,  0.9653],\n",
       "         [ 0.4970, -0.4172]],\n",
       "\n",
       "        [[-0.5505,  0.9653],\n",
       "         [ 0.4970, -0.4172],\n",
       "         [ 0.6375,  0.2702]],\n",
       "\n",
       "        [[ 0.4970, -0.4172],\n",
       "         [ 0.6375,  0.2702],\n",
       "         [ 2.1014,  1.1716]],\n",
       "\n",
       "        [[ 0.6375,  0.2702],\n",
       "         [ 2.1014,  1.1716],\n",
       "         [-0.1359,  0.9548]],\n",
       "\n",
       "        [[ 2.1014,  1.1716],\n",
       "         [-0.1359,  0.9548],\n",
       "         [ 0.0332, -0.0345]],\n",
       "\n",
       "        [[-0.1359,  0.9548],\n",
       "         [ 0.0332, -0.0345],\n",
       "         [ 0.5172, -1.0980]]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape # (number of examples, block size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5505,  0.9653],\n",
       "        [-0.5505,  0.9653],\n",
       "        [-0.5505,  0.9653]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 2])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape # (27, 2) embedding matrix for 27 characters with 2-dimensional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape # embedding for all characters in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[13][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1712, -0.7044])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[stoi[names[13][2]]] # embedding for character 'a' in name 'aab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5172, -1.0980])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X][13,2] # embedding for the 13th example, 2nd character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5172, -1.0980])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X[13][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..a"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(itos[X[13][i].item()], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 2])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is C[1] and how it is equivalent to C[X[13][1]]?\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embbeding and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] # embedding for all characters in X\n",
    "emb.shape # (number of examples, block size, embedding dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5505,  0.9653],\n",
       "        [-0.5505,  0.9653],\n",
       "        [-0.5505,  0.9653]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[0] # embedding of ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5505,  0.9653],\n",
       "        [-0.5505,  0.9653],\n",
       "        [ 0.1676, -0.3755]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[1] # embedding of the ..e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1676, -0.3755],\n",
       "        [-0.3721, -0.7949],\n",
       "        [-0.3721, -0.7949]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[3] # embedding of the emm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding are vector given to a certain word and in above we have 3 input to NN as block size is 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start the main Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 3 block and each block has 1 character with 2 embedding dimension so the total embedding dimension is 3*2=6\n",
    "W1 = torch.randn((6, 100)) # 6-dimensional embedding to 100-dimensional hidden layer\n",
    "b1 = torch.randn(100) # bias for hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[131]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43memb\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m + b1 \u001b[38;5;66;03m# linear transformation from embedding to hidden layer\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)"
     ]
    }
   ],
   "source": [
    "# emb @ W1 + b1 # linear transformation from embedding to hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to concatenate embeddings along dimension 1 (which should match in other dimensions):\n",
    "torch.cat([emb[:,0,:],emb[:,1,:],emb[:,2,:]],1).shape  # Concatenates first 3 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(torch.unbind(emb,1),1).shape # Concatenates first 3 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.cat([emb[:,0,:],emb[:,1,:],emb[:,2,:]],1)==torch.cat(torch.unbind(emb,1),1)).all() # Check if they are equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Advance feature of flatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(18) # random tensor of shape (3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5086, -0.6824],\n",
       "         [ 0.7816, -0.8055],\n",
       "         [-0.7320, -1.0116]],\n",
       "\n",
       "        [[-0.1844,  0.1980],\n",
       "         [-0.1459,  1.4125],\n",
       "         [ 0.8560, -1.5703]],\n",
       "\n",
       "        [[-1.1208,  0.0909],\n",
       "         [-1.1916, -0.0298],\n",
       "         [ 1.9229,  0.1539]]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(3,3,2) # reshape to (3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prath\\AppData\\Local\\Temp\\ipykernel_18008\\214256462.py:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  a.storage()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 0.5085802674293518\n",
       " -0.6823678016662598\n",
       " 0.7815552949905396\n",
       " -0.8054583668708801\n",
       " -0.7320087552070618\n",
       " -1.0115565061569214\n",
       " -0.1844131499528885\n",
       " 0.19803953170776367\n",
       " -0.1458822786808014\n",
       " 1.4125443696975708\n",
       " 0.8559510707855225\n",
       " -1.570260763168335\n",
       " -1.1207613945007324\n",
       " 0.09086468070745468\n",
       " -1.1915953159332275\n",
       " -0.029782384634017944\n",
       " 1.9229425191879272\n",
       " 0.15391285717487335\n",
       "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 18]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5505,  0.9653, -0.5505,  0.9653, -0.5505,  0.9653],\n",
       "        [-0.5505,  0.9653, -0.5505,  0.9653,  0.1676, -0.3755],\n",
       "        [-0.5505,  0.9653,  0.1676, -0.3755, -0.3721, -0.7949],\n",
       "        [ 0.1676, -0.3755, -0.3721, -0.7949, -0.3721, -0.7949],\n",
       "        [-0.3721, -0.7949, -0.3721, -0.7949,  0.5172, -1.0980],\n",
       "        [-0.5505,  0.9653, -0.5505,  0.9653, -0.5505,  0.9653],\n",
       "        [-0.5505,  0.9653, -0.5505,  0.9653,  0.6375,  0.2702],\n",
       "        [-0.5505,  0.9653,  0.6375,  0.2702,  0.1712, -0.7044],\n",
       "        [ 0.6375,  0.2702,  0.1712, -0.7044,  0.0332, -0.0345],\n",
       "        [ 0.1712, -0.7044,  0.0332, -0.0345,  0.5857,  0.6051],\n",
       "        [ 0.0332, -0.0345,  0.5857,  0.6051,  0.0332, -0.0345],\n",
       "        [ 0.5857,  0.6051,  0.0332, -0.0345,  0.5172, -1.0980],\n",
       "        [-0.5505,  0.9653, -0.5505,  0.9653, -0.5505,  0.9653],\n",
       "        [-0.5505,  0.9653, -0.5505,  0.9653,  0.5172, -1.0980],\n",
       "        [-0.5505,  0.9653,  0.5172, -1.0980,  0.5857,  0.6051],\n",
       "        [ 0.5172, -1.0980,  0.5857,  0.6051,  0.5172, -1.0980],\n",
       "        [-0.5505,  0.9653, -0.5505,  0.9653, -0.5505,  0.9653],\n",
       "        [-0.5505,  0.9653, -0.5505,  0.9653,  0.0332, -0.0345],\n",
       "        [-0.5505,  0.9653,  0.0332, -0.0345,  0.4970, -0.4172],\n",
       "        [ 0.0332, -0.0345,  0.4970, -0.4172,  0.5172, -1.0980],\n",
       "        [ 0.4970, -0.4172,  0.5172, -1.0980, -0.5991, -0.7920],\n",
       "        [ 0.5172, -1.0980, -0.5991, -0.7920,  0.1676, -0.3755],\n",
       "        [-0.5991, -0.7920,  0.1676, -0.3755,  0.1712, -0.7044],\n",
       "        [ 0.1676, -0.3755,  0.1712, -0.7044,  0.1712, -0.7044],\n",
       "        [ 0.1712, -0.7044,  0.1712, -0.7044,  0.5172, -1.0980],\n",
       "        [-0.5505,  0.9653, -0.5505,  0.9653, -0.5505,  0.9653],\n",
       "        [-0.5505,  0.9653, -0.5505,  0.9653,  0.4970, -0.4172],\n",
       "        [-0.5505,  0.9653,  0.4970, -0.4172,  0.6375,  0.2702],\n",
       "        [ 0.4970, -0.4172,  0.6375,  0.2702,  2.1014,  1.1716],\n",
       "        [ 0.6375,  0.2702,  2.1014,  1.1716, -0.1359,  0.9548],\n",
       "        [ 2.1014,  1.1716, -0.1359,  0.9548,  0.0332, -0.0345],\n",
       "        [-0.1359,  0.9548,  0.0332, -0.0345,  0.5172, -1.0980]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(32, 6) # reshape to (32, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(32, 6) == torch.cat(torch.unbind(emb,1),1) # Check if they are equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continue with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = emb.view(emb.shape[0],6) @ W1 + b1 # linear transformation from embedding to hidden layer\n",
    "# alternative way to do the same thing\n",
    "h = torch.tanh(emb.view(-1,6) @ W1 + b1) # linear transformation from embedding to hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9964,  0.5287,  0.9678,  ..., -0.7641,  0.3293,  0.1134],\n",
       "        [-0.9962, -0.3652,  0.7386,  ..., -0.9020, -0.1333,  0.0140],\n",
       "        [-0.6333, -0.1956, -0.9738,  ..., -0.6146,  0.8490, -0.3872],\n",
       "        ...,\n",
       "        [ 0.8839,  0.8141, -0.9991,  ...,  0.9997, -0.1372,  0.9892],\n",
       "        [ 0.9955, -0.9994, -0.8716,  ..., -0.0252,  0.8697,  0.9970],\n",
       "        [-0.4507, -0.9166, -0.9165,  ..., -0.8108,  0.5001, -0.0135]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape # (number of examples, hidden layer size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "### warning: while and the bias the dimention wise adding might be wrong\n",
    "# 32 ,100\n",
    "# 1(fake dimension) ,100 in b1 as bias was 100 in dimension and while adding the dimension align to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100, 27)) # 100-dimensional hidden layer to 27-dimensional output layer as we have 27 characters\n",
    "b2 = torch.randn(27) # bias for output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = h @ W2 + b2 # linear transformation from hidden layer to output layer\n",
    "logits.shape # (number of examples, output layer size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.7007)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counts = logits.exp() # exponentiate the logits to get counts\n",
    "# prob = counts / counts.sum(1, keepdim=True) # normalize to get probabilities\n",
    "# prob.shape # (number of examples, output layer size)\n",
    "# loss = -prob[torch.arange(32), Y].log().mean() # negative log likelihood loss\n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.7007)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# above is equivalent to the following code\n",
    "loss = F.cross_entropy(logits, Y) # cross entropy loss and use this instead of the above code as it is more efficient and numerically stable\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(32) # 32 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.8424e-08, 6.3380e-13, 1.9539e-09, 1.0754e-08, 2.6381e-08, 1.1545e-06,\n",
       "        7.3593e-08, 7.4603e-10, 4.9412e-11, 5.2409e-07, 3.3851e-04, 1.3655e-05,\n",
       "        1.3326e-07, 7.5386e-05, 1.8526e-12, 2.0684e-06, 2.9216e-04, 1.4561e-07,\n",
       "        1.3808e-12, 2.5120e-07, 1.5755e-05, 2.6834e-11, 2.6447e-10, 3.3107e-08,\n",
       "        1.2383e-06, 5.8305e-07, 1.1109e-05, 6.3519e-10, 7.9154e-01, 2.5525e-04,\n",
       "        2.3577e-05, 9.5987e-04])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prob[torch.arange(32), Y] # probabilities of the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
